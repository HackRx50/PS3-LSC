{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaahilShaikh17/Forgery-Detection-Hackrx/blob/main/hackrx_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3ZkkHrZA_yd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm3ivN7yBe0I",
        "outputId": "4aa786cd-80e0-4a94-93bb-c2b47c8e554d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount the Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths for genuine and forged folders in your Google Drive\n",
        "base_dir = '/content/drive/My Drive/Bajaj-HackRX/datasets'\n",
        "\n",
        "\n",
        "forged_dir = os.path.join(base_dir, 'forged')\n",
        "genuine_dir = os.path.join(base_dir, 'genuine')\n",
        "\n",
        "preprocessed_forged_dir = os.path.join(base_dir, 'preprocessed_forged')\n",
        "preprocessed_genuine_dir = os.path.join(base_dir, 'preprocessed_genuine')\n",
        "\n",
        "augmented_forged_dir = os.path.join(base_dir, 'augmented_forged')\n",
        "augmented_genuine_dir = os.path.join(base_dir, 'augmented_genuine')\n",
        "\n",
        "# Create base output directories if they don't exist\n",
        "os.makedirs(preprocessed_forged_dir, exist_ok=True)\n",
        "os.makedirs(preprocessed_genuine_dir, exist_ok=True)\n",
        "os.makedirs(augmented_forged_dir, exist_ok=True) # Augemented forged dir\n",
        "os.makedirs(augmented_genuine_dir, exist_ok=True) # Augemented genuine dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "--HoWBbdBz2O",
        "outputId": "83c5f8ba-6c95-44f0-d84d-c6cc6a4976a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nusing 2048x2048 so that all images have the same dimensions, making it easier\\nfor further preprocessing and model training\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parameters for resizing\n",
        "resize_dims = (2048, 2048)\n",
        "'''\n",
        "using 2048x2048 so that all images have the same dimensions, making it easier\n",
        "for further preprocessing and model training\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VMtrquilCSfK"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess an image\n",
        "def preprocess_image(image_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Rescaling: Resize the image\n",
        "    image_resized = cv2.resize(image, resize_dims, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Denoising: Use Bilateral filtering (you can change to Gaussian or Median if needed)\n",
        "    image_denoised = cv2.bilateralFilter(image_resized, d=9, sigmaColor=75, sigmaSpace=75)\n",
        "\n",
        "    ''' The commented code is to denoise and adjust the contrast the image '''\n",
        "    # # Contrast Adjustment: Histogram Equalization (convert to grayscale first)\n",
        "    # image_gray = cv2.cvtColor(image_denoised, cv2.COLOR_BGR2GRAY)\n",
        "    # image_equalized = cv2.equalizeHist(image_gray)\n",
        "\n",
        "    # Convert back to BGR format\n",
        "    # image_preprocessed = cv2.cvtColor(image_equalized, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    return image_denoised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O09eOYihG7qJ"
      },
      "outputs": [],
      "source": [
        "# Function to process images in a directory and save to corresponding output folder\n",
        "def process_images(input_dir, output_dir):\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        # Create corresponding output directories\n",
        "        rel_path = os.path.relpath(root, input_dir)  # Relative path from input_dir\n",
        "        output_subdir = os.path.join(output_dir, rel_path)  # Corresponding subdir in output_dir\n",
        "        os.makedirs(output_subdir, exist_ok=True)  # Create the subdir if it doesn't exist\n",
        "\n",
        "        for file in files:\n",
        "            if file.endswith(('.tif', '.png')):  # Only process .tif and .png images\n",
        "                input_image_path = os.path.join(root, file)\n",
        "\n",
        "                # Modify filename by appending 'p' before the extension\n",
        "                file_name, file_ext = os.path.splitext(file)\n",
        "                new_file_name = f\"{file_name}p{file_ext}\"\n",
        "\n",
        "                output_image_path = os.path.join(output_subdir, new_file_name)\n",
        "\n",
        "                # Preprocess and save the image\n",
        "                preprocessed_image = preprocess_image(input_image_path)\n",
        "                if preprocessed_image is not None:\n",
        "                    cv2.imwrite(output_image_path, preprocessed_image)\n",
        "                    print(f\"Saved preprocessed image: {output_image_path}\")\n",
        "\n",
        "# Process forged images and save in preprocessed_forged with the same subfolder structure\n",
        "process_images(forged_dir, preprocessed_forged_dir)\n",
        "\n",
        "# Process genuine images and save in preprocessed_genuine with the same subfolder structure\n",
        "process_images(genuine_dir, preprocessed_genuine_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Forged Augmentation"
      ],
      "metadata": {
        "id": "_3GObWNVikv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "\n",
        "# def load_images_from_folder(folder_path):\n",
        "#     images = []\n",
        "#     for filename in os.listdir(folder_path):\n",
        "#         img_path = os.path.join(folder_path, filename)\n",
        "#         img = Image.open(img_path)\n",
        "#         images.append(np.array(img))\n",
        "#     return images\n",
        "\n",
        "# preprocessed_images = load_images_from_folder('https://drive.google.com/drive/folders/1N-CVvCMGkkW4iA5qjo9hT3gFpq9Ala1H?usp=drive_link')\n"
      ],
      "metadata": {
        "id": "vTIu0iD1kGpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set Up Data Augmentation"
      ],
      "metadata": {
        "id": "ll2x_5fjkPiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "yfPPuovCkLf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply Augmentation and Save Images"
      ],
      "metadata": {
        "id": "MEGQduGxkT-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create output directory if it doesn't exist\n",
        "# def augment_preprocessed_forged(input_dir, output_dir):\n",
        "#   # if not os.path.exists(output_folder):\n",
        "#   #     os.makedirs(output_folder)\n",
        "#   datagen = ImageDataGenerator(\n",
        "#         rotation_range=40,\n",
        "#         width_shift_range=0.2,\n",
        "#         height_shift_range=0.2,\n",
        "#         shear_range=0.2,\n",
        "#         zoom_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "#         fill_mode='nearest'\n",
        "#     )\n",
        "\n",
        "#   for root, dirs, files in os.walk(input_dir):\n",
        "#     rel_path = os.path.relpath(root, input_dir)  # Relative path from input_dir\n",
        "#     output_subdir = os.path.join(output_dir, rel_path)  # Corresponding subdir in output_dir\n",
        "#     os.makedirs(output_subdir, exist_ok=True)  # Create the subdir if it doesn't exist\n",
        "\n",
        "#   for file in files:\n",
        "#     file_path = os.path.join(root, file)\n",
        "#     img = Image.open(file_path)\n",
        "#     img_array = np.array(img)\n",
        "#     img_array = np.expand_dims(img_array, 0)  # Add batch dimension\n",
        "\n",
        "#     # Apply augmentation and save images\n",
        "#     for batch in datagen.flow(img_array, batch_size=1):\n",
        "#         augmented_image = batch[0].astype('uint8')  # Convert back to uint8\n",
        "#         augmented_img = Image.fromarray(augmented_image)\n",
        "\n",
        "#         # Save augmented image\n",
        "#         augmented_img_path = os.path.join(output_subdir, f'augmented_{file}')\n",
        "#         augmented_img.save(augmented_img_path)\n",
        "#         break  # Stop after saving one augmented image per original image"
      ],
      "metadata": {
        "id": "0AAWIsKbkTyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def augment_preprocessed_forged(input_dir, output_dir):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        rel_path = os.path.relpath(root, input_dir)  # Relative path from input_dir\n",
        "        output_subdir = os.path.join(output_dir, rel_path)  # Corresponding subdir in output_dir\n",
        "        os.makedirs(output_subdir, exist_ok=True)  # Create the subdir if it doesn't exist\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            img = Image.open(file_path)\n",
        "            img_array = np.array(img)\n",
        "            img_array = np.expand_dims(img_array, 0)  # Add batch dimension\n",
        "\n",
        "            # Apply augmentation and save images\n",
        "            for batch in datagen.flow(img_array, batch_size=1):\n",
        "                augmented_image = batch[0].astype('uint8')  # Convert back to uint8\n",
        "                augmented_img = Image.fromarray(augmented_image)\n",
        "\n",
        "                # Save augmented image\n",
        "                augmented_img_path = os.path.join(output_subdir, f'augmented_{file}')\n",
        "                augmented_img.save(augmented_img_path)\n",
        "                break  # Stop after saving one augmented image per original image\n",
        "\n",
        "# Define your directories\n",
        "# base_dir = '/content/drive/My Drive/Bajaj-HackRX/datasets'\n",
        "input_dir = os.path.join(base_dir, 'preprocessed_forged')\n",
        "augmented_forged_dir = os.path.join(base_dir, 'augmented_forged')\n",
        "\n",
        "# Call the function\n",
        "augment_preprocessed_forged(input_dir, augmented_forged_dir)\n"
      ],
      "metadata": {
        "id": "u-Zo4JD_n2nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def augment_preprocessed_forged(input_dir, output_dir):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        rel_path = os.path.relpath(root, input_dir)  # Relative path from input_dir\n",
        "        output_subdir = os.path.join(output_dir, rel_path)  # Corresponding subdir in output_dir\n",
        "        os.makedirs(output_subdir, exist_ok=True)  # Create the subdir if it doesn't exist\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            img = Image.open(file_path)\n",
        "            img_array = np.array(img)\n",
        "            img_array = np.expand_dims(img_array, 0)  # Add batch dimension\n",
        "\n",
        "            # Apply augmentation and save images\n",
        "            for batch in datagen.flow(img_array, batch_size=1):\n",
        "                augmented_image = batch[0].astype('uint8')  # Convert back to uint8\n",
        "                augmented_img = Image.fromarray(augmented_image)\n",
        "\n",
        "                # Save augmented image\n",
        "                augmented_img_path = os.path.join(output_subdir, f'augmented_{file}')\n",
        "                augmented_img.save(augmented_img_path)\n",
        "                break  # Stop after saving one augmented image per original image\n",
        "\n",
        "# Define your directories\n",
        "# base_dir = '/content/drive/My Drive/Bajaj-HackRX/datasets'\n",
        "input_dir_genuine = os.path.join(base_dir, 'preprocessed_genuine')\n",
        "augmented_genuine_dir = os.path.join(base_dir, 'augmented_genuine')\n",
        "\n",
        "# Call the function\n",
        "augment_preprocessed_forged(input_dir_genuine, augmented_genuine_dir)\n"
      ],
      "metadata": {
        "id": "41OfTcySm0t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Train, Test split"
      ],
      "metadata": {
        "id": "k7r7L4v3ryt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image data generator with 70-30 train-test split\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)  # Normalize images\n",
        "\n",
        "# Load training set (70%)\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(224, 224),  # Resize images if needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Use 'binary' if you have 2 classes\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Load test set (30%)\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "id": "_4a_w05eeoIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0938965b-d4b0-4ff8-f068-8954cdeec585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 177 images belonging to 8 classes.\n",
            "Found 75 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')  # For multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use binary_crossentropy for binary classification\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "1MhpLXDMesWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46649fb-2533-4956-cdc7-2d47303431cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=test_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcueC1ybfMMc",
        "outputId": "480a4b7e-ca08-4f4a-c568-a386b3c9a299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 12s/step - accuracy: 0.4599 - loss: 3.5252 - val_accuracy: 0.7067 - val_loss: 1.0407\n",
            "Epoch 2/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.6534 - loss: 1.1138 - val_accuracy: 0.7067 - val_loss: 1.1596\n",
            "Epoch 3/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5s/step - accuracy: 0.6819 - loss: 1.1560 - val_accuracy: 0.7067 - val_loss: 1.0136\n",
            "Epoch 4/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5s/step - accuracy: 0.6837 - loss: 0.9733 - val_accuracy: 0.7067 - val_loss: 0.9617\n",
            "Epoch 5/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 0.7250 - loss: 0.7339 - val_accuracy: 0.7067 - val_loss: 1.1176\n",
            "Epoch 6/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5s/step - accuracy: 0.7062 - loss: 0.6838 - val_accuracy: 0.5600 - val_loss: 1.1928\n",
            "Epoch 7/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 4s/step - accuracy: 0.7267 - loss: 0.6436 - val_accuracy: 0.6933 - val_loss: 1.1025\n",
            "Epoch 8/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.7578 - loss: 0.6072 - val_accuracy: 0.4933 - val_loss: 1.1046\n",
            "Epoch 9/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 4s/step - accuracy: 0.7717 - loss: 0.5103 - val_accuracy: 0.7200 - val_loss: 0.8659\n",
            "Epoch 10/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - accuracy: 0.7956 - loss: 0.3934 - val_accuracy: 0.4400 - val_loss: 1.0833\n",
            "Epoch 11/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.8133 - loss: 0.3891 - val_accuracy: 0.7200 - val_loss: 0.8311\n",
            "Epoch 12/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4s/step - accuracy: 0.8343 - loss: 0.4161 - val_accuracy: 0.5467 - val_loss: 0.9114\n",
            "Epoch 13/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - accuracy: 0.8668 - loss: 0.3015 - val_accuracy: 0.7200 - val_loss: 1.1164\n",
            "Epoch 14/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 5s/step - accuracy: 0.8446 - loss: 0.3787 - val_accuracy: 0.6000 - val_loss: 0.9381\n",
            "Epoch 15/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5s/step - accuracy: 0.8679 - loss: 0.2554 - val_accuracy: 0.6800 - val_loss: 0.8064\n",
            "Epoch 16/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 7s/step - accuracy: 0.9185 - loss: 0.2289 - val_accuracy: 0.7200 - val_loss: 0.9401\n",
            "Epoch 17/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 5s/step - accuracy: 0.8852 - loss: 0.2138 - val_accuracy: 0.7200 - val_loss: 1.1266\n",
            "Epoch 18/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - accuracy: 0.8932 - loss: 0.3059 - val_accuracy: 0.5867 - val_loss: 0.8724\n",
            "Epoch 19/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 4s/step - accuracy: 0.9718 - loss: 0.1978 - val_accuracy: 0.6933 - val_loss: 0.8706\n",
            "Epoch 20/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 4s/step - accuracy: 0.9289 - loss: 0.1505 - val_accuracy: 0.6000 - val_loss: 0.9831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gvQTXe5fQAY",
        "outputId": "c6947910-bca2-4a00-da48-058bce1869b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - accuracy: 0.6047 - loss: 0.9521\n",
            "Test accuracy: 0.6000000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Add dropout to prevent overfitting\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # L2 regularization\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "LxMOpZtpkT1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jr-R_IdsGnv",
        "outputId": "c3146f80-60d6-40e1-f065-994f28f7a70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.3849 - loss: 4.4068 - val_accuracy: 0.7067 - val_loss: 1.3376\n",
            "Epoch 2/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 894ms/step - accuracy: 0.7066 - loss: 1.3061 - val_accuracy: 0.7067 - val_loss: 1.1992\n",
            "Epoch 3/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 864ms/step - accuracy: 0.6763 - loss: 1.2420 - val_accuracy: 0.7067 - val_loss: 1.2383\n",
            "Epoch 4/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 876ms/step - accuracy: 0.6490 - loss: 1.1830 - val_accuracy: 0.7067 - val_loss: 1.2853\n",
            "Epoch 5/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6898 - loss: 0.9635 - val_accuracy: 0.7067 - val_loss: 1.3515\n",
            "Epoch 6/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 929ms/step - accuracy: 0.6719 - loss: 0.9435 - val_accuracy: 0.7067 - val_loss: 1.3304\n",
            "Epoch 7/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 998ms/step - accuracy: 0.6791 - loss: 0.9763 - val_accuracy: 0.7067 - val_loss: 1.3248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP8JaI-GzwXv",
        "outputId": "1d4f12b1-8f78-4a33-9f32-1778b529f132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.7205 - loss: 1.1876\n",
            "Test accuracy: 0.7066666483879089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObVFbpNXsgrV",
        "outputId": "0f3aabf8-473b-4479-f406-f3c4abe2114f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.6785 - loss: 1.2255 - val_accuracy: 0.7067 - val_loss: 1.1351 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.7288 - loss: 1.0487 - val_accuracy: 0.7067 - val_loss: 1.1533 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.7168 - loss: 0.9501 - val_accuracy: 0.6133 - val_loss: 1.3502 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 933ms/step - accuracy: 0.7331 - loss: 0.7630 - val_accuracy: 0.7067 - val_loss: 1.1982 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.7535 - loss: 0.8018 - val_accuracy: 0.7200 - val_loss: 1.1985 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.7720 - loss: 0.6813 - val_accuracy: 0.7200 - val_loss: 1.2193 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itCRhHaQz2-m",
        "outputId": "1c1b7aa8-631f-4098-9820-bb3ada86f1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.7479 - loss: 1.0507\n",
            "Test accuracy: 0.7066666483879089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = models.Sequential([\n",
        "#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.MaxPooling2D((2, 2)),\n",
        "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.MaxPooling2D((2, 2)),\n",
        "#     layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "#     layers.Flatten(),\n",
        "#     layers.Dense(128, activation='relu'),\n",
        "#     layers.BatchNormalization(),\n",
        "#     layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "# ])\n"
      ],
      "metadata": {
        "id": "voHXpwJ6yz6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# Define the base directory\n",
        "b_dir = '/content/drive/My Drive/Bajaj-HackRX/datasets/Final_data'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)  # Normalize images\n",
        "\n",
        "\n",
        "# Training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    b_dir,\n",
        "    target_size=(224, 224),  # ResNet50 input size\n",
        "    batch_size=32,\n",
        "    class_mode='binary',     # Binary classification\n",
        "    subset='training',\n",
        "    shuffle=True)\n",
        "\n",
        "# Validation data generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    b_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    shuffle=False)\n"
      ],
      "metadata": {
        "id": "legE2M9Sy48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b39dab-e2ba-411f-b14c-33df2d3577e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 273 images belonging to 2 classes.\n",
            "Found 116 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model on top\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # For binary classification\n",
        "])\n"
      ],
      "metadata": {
        "id": "03X1b6aUrkKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n"
      ],
      "metadata": {
        "id": "sqMzvLhssiVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0zrsR00slx4",
        "outputId": "6ecc0b0b-3f0f-4203-b6bf-39f27915d966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 11s/step - accuracy: 0.9332 - loss: 0.3058 - precision: 0.9583 - recall: 0.9732 - val_accuracy: 0.9483 - val_loss: 0.2045 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 2/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 973ms/step - accuracy: 0.9526 - loss: 0.2050 - precision: 0.9526 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2082 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 3/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 968ms/step - accuracy: 0.9469 - loss: 0.2124 - precision: 0.9469 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2121 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 4/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 981ms/step - accuracy: 0.9461 - loss: 0.2248 - precision: 0.9461 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2092 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 5/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 770ms/step - accuracy: 0.9215 - loss: 0.3376 - precision: 0.9215 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2029 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 6/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 931ms/step - accuracy: 0.9577 - loss: 0.1822 - precision: 0.9577 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2021 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 7/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9256 - loss: 0.2870 - precision: 0.9256 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2009 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 8/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 953ms/step - accuracy: 0.9396 - loss: 0.2444 - precision: 0.9396 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.2006 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 9/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 993ms/step - accuracy: 0.9468 - loss: 0.2298 - precision: 0.9479 - recall: 0.9988 - val_accuracy: 0.9483 - val_loss: 0.1995 - val_precision: 0.9483 - val_recall: 1.0000\n",
            "Epoch 10/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 970ms/step - accuracy: 0.9421 - loss: 0.2468 - precision: 0.9421 - recall: 1.0000 - val_accuracy: 0.9483 - val_loss: 0.1989 - val_precision: 0.9483 - val_recall: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation data\n",
        "val_loss, val_accuracy, val_precision, val_recall = model.evaluate(validation_generator)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "print(f\"Validation Precision: {val_precision}\")\n",
        "print(f\"Validation Recall: {val_recall}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "202JpYY4ta0X",
        "outputId": "e60f6a2a-04b2-4c67-b13b-3a6737796dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9106 - loss: 0.3135 - precision: 0.9106 - recall: 1.0000\n",
            "Validation Accuracy: 0.9482758641242981\n",
            "Validation Precision: 0.9482758641242981\n",
            "Validation Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJ58AePFvkLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}